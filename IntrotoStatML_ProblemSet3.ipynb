{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6cb977",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Read the following highly-cited article by Fearon and Laitin (2003):\n",
    "https://cisac.fsi.stanford.edu/publications/ethnicity_insurgency_and_civil_war\n",
    "\n",
    "Answer the following:\n",
    "\n",
    "a. What is the paper about? (Please write a paragraph â€“ max. 250 words)\n",
    "\n",
    "b. How many observations do authors have in their dataset? What does each\n",
    "observation represent? (=What is the unit of analysis?)\n",
    "\n",
    "c. What is the identification strategy of the authors? (=What are the different\n",
    "regression equations they are running?) Please write down in form of equations\n",
    "and explain. Identify the independent and dependent variables.\n",
    "\n",
    "d. What do the coefficient values listed in Table 1 represent? (theoretically speaking)\n",
    "\n",
    "e. Which independent variables have positive coefficients? Which independent\n",
    "variables have negative coefficients? Which ones are statistically significant?\n",
    "\n",
    "f. Thinking about the range of your independent variables, which variables do you\n",
    "think have a greater impact on the dependent variable(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c64be",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8639a6",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "The authors of the paper, James D. Fearon and David D. Laitin study and evaluate the possible root causes of the many civil wars that took place in the United States after the Cold War between the mid to late 1900s. They perform various analyses using factors which might be possible influences on civil strife such as Per Capita Income, Ethnic and Religious Composition, Democracy and Civil Liberties, among many others.\n",
    "They also disprove a few commonly believed theories such as the civil wars taking place due to changes in the international system, more ethnic and religious diversity causing such civil conflicts and the ability to predict the occurence of a civil war based on the strength of political and ethnic grievances of the people of a nation. They lay out many such hypotheses based on common arguments and rebut them using solid evidence they gathered from their data. Moreover, they ellaborate on what common misconception might have led to such false notions regarding the matter.\n",
    "They conclude several interesting things at the end of the study such as the conditions favoring civil wars not being ethnic and religious diversity against popular belief. In fact, conditions that favour insurgency were found to be state weakness [\"Decolonization from the 1940s through the 1970s gave birth to a large number of financially, bureaucratically, and militarily weak states\"] indicated by the population, poverty and instability within a state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c6e72",
   "metadata": {},
   "source": [
    "### Part b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f33714",
   "metadata": {},
   "source": [
    "The authors had data on about 127 civil war starts in a sample of 6,610 country years.\n",
    "\n",
    "The unit of analysis refers to the main parameter that one is investigating in a research project or study. The authors' objective was to study the susceptibility of a country to a civil war. Hence, the unit of analysis in the study are countries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c886713",
   "metadata": {},
   "source": [
    "### Part c\n",
    "\n",
    "control variable: Prior war - $ x_1 $\n",
    "\n",
    "independent variables: \n",
    "\n",
    "1. Per capita income  - $ x_2 $ \n",
    "2. log(population) - $ x_3 $\n",
    "3. log(% mountainous)- $ x_4 $\n",
    "4. Noncontiguous state - $ x_5 $\n",
    "5. Oil exporter - $ x_6 $\n",
    "6. New state - $ x_7 $\n",
    "7. Instability - $ x_8 $\n",
    "8. Democracy (Polity IV) - $ x_9 $\n",
    "9. Ethnic fractionalization  - $ x_{10} $\n",
    "10. Religious fractionalization - $ x_{11} $\n",
    "11. Anocracy - $ x_{12} $\n",
    "12. Democracy (Dichotomous) - $ x_{13} $\n",
    "\n",
    "dependent variable: onset - $ Y $\n",
    "\n",
    "The following regression equations were ran by the authors:\n",
    "\n",
    "Model 1:\n",
    "\n",
    "$ -6.731 - (0.954*x_1) - (0.344*x_2) + (0.263*x_3) + (0.219*x_4) + (0.443*x_5) + (0.858*x_6) + \\\\\n",
    "(1.709*x_7) + (0.618*x_8) + (0.021*x_9) + (0.166*x_{10}) + (0.285*x_{11}) $\n",
    "\n",
    "Model 2:\n",
    "\n",
    "$ -8.450 - (0.849*x_1) - (0.379*x_2) + (0.389*x_3) + (0.120*x_4) + (0.481*x_5) + (0.809*x_6) + \\\\\n",
    "(1.777*x_7) + (0.385*x_8) + (0.013*x_9) + (0.146*x_{10}) + (1.533*x_{11}) $\n",
    "\n",
    "Model 3:\n",
    "\n",
    "$ -7.019 - (0.916*x_1) - (0.318*x_2) + (0.272 *x_3) + (0.199 *x_4) + (0.426 *x_5) + \\\\\n",
    "(0.751*x_6) + (1.658*x_7) + (0.513*x_8) + (0.164*x_{10}) + (0.326*x_{11}) + \n",
    "(0.521*x_{12}) + (0.127*x_{13}) $\n",
    "\n",
    "Model 4:\n",
    "\n",
    "$ -7.019 - (0.688*x_1) - (0.305*x_2) + (0.267 *x_3) + (0.192 *x_4) + (0.798 *x_5) + \\\\\n",
    "(0.548*x_6) + (1.523*x_7) + (0.548*x_8) + (0.490*x_{10}) $\n",
    "\n",
    "Model 5:\n",
    "\n",
    "$ -6.801 - (0.551*x_1) - (0.309*x_2) + (0.223*x_3) + (0.418*x_4) - (0.171*x_5) + \\\\\n",
    "(1.269*x_6) + (1.147*x_7) + (0.584*x_8) - (0.119*x_9) + (1.176*x_{11}) + \\\\\n",
    "(0.597*x_{12}) + (0.219*x_{13}) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fcc1dd",
   "metadata": {},
   "source": [
    "### Part d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88ca0c",
   "metadata": {},
   "source": [
    "Generally speaking, the coefficients describe the relationship between the independent variables and the dependent variable. \n",
    "\n",
    "In this scenario, a high positive coefficent means that an increase in the value of the variable with the high positive coefficient will increase the likelihood of the war occuring. Similarly, a high negative coefficient would mean that an increase in the value of the independent variable with the high negative coefficient would entail a decrease in the likelihood of the war occuring.\n",
    "\n",
    "For example, the independent variable \"Instability\" has a high positive coefficient, this means that theoretically, an increase in the political instability of antion would increase the likelihood of a war occuring. And, the negative coefficient for the independent variable \"Per Capita Income\" suggests that an increase in the per capita income of the nation would decrease the likelihood of a war occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da76de",
   "metadata": {},
   "source": [
    "### Part e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf890b",
   "metadata": {},
   "source": [
    "Independent variables with positive coeffiecients:\n",
    "\n",
    "1. log(population) \n",
    "2. log(% mountainous)\n",
    "3. Noncontiguous state \n",
    "4. Oil exporter\n",
    "5. New state\n",
    "6. Instability\n",
    "7. Democracy (Polity IV) \n",
    "8. Ethnic fractionalization \n",
    "9. Religious fractionalization \n",
    "10. Anocracy \n",
    "11. Democracy (Dichotomous)\n",
    "\n",
    "Independent varaibles with negative coefficients:\n",
    "\n",
    "1. Prior war \n",
    "2. Per capita income\n",
    "\n",
    "\n",
    "Consider the following models:\n",
    "\n",
    "1 - Civil War \n",
    "2 - Ethnic War \n",
    "3 - Civil War\n",
    "4 - Civil War (Plus Empires) \n",
    "5 - Civil War(COW)\n",
    "\n",
    "The confidence levels were set at 95%. The following variables are statistically significant for each model respectively:\n",
    "\n",
    "Model 1: \n",
    "\n",
    "1. Prior war \n",
    "2. Per capita income\n",
    "3. log(population) \n",
    "4. log(% mountainous)\n",
    "5. Oil exporter\n",
    "6. New state\n",
    "7. Instability\n",
    "\n",
    "Model 2:\n",
    "\n",
    "1. Prior war \n",
    "2. Per capita income\n",
    "3. log(population) \n",
    "4. Oil exporter\n",
    "5. New state\n",
    "6. Religious fractionalization\n",
    "\n",
    "Model 3:\n",
    "\n",
    "1. Prior war \n",
    "2. Per capita income\n",
    "3. log(population) \n",
    "4. log(% mountainous)\n",
    "5. Oil exporter\n",
    "6. New state\n",
    "7. Instability\n",
    "8. Anocracy\n",
    "\n",
    "Model 4:\n",
    "\n",
    "1. Prior war \n",
    "2. Per capita income\n",
    "3. log(population) \n",
    "4. log(% mountainous)\n",
    "5. Noncontiguous state\n",
    "6. Oil exporter\n",
    "7. New state\n",
    "8. Instability\n",
    "\n",
    "Model 5:\n",
    "\n",
    "1. Per capita income\n",
    "2. log(population) \n",
    "3. log(% mountainous)\n",
    "4. Oil exporter\n",
    "5. New state\n",
    "6. Instability\n",
    "7. Religious fractionalization\n",
    "8. Anocracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccc83f",
   "metadata": {},
   "source": [
    "### Part f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ccca0",
   "metadata": {},
   "source": [
    "The independent variables that have the greatest impact on the dependent variable are:\n",
    "\n",
    "1. Per capita income\n",
    "2. log(population)\n",
    "3. New state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960bb78",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb5151",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    "Build a two-class logistic regression model from scratch. You will need to work on the\n",
    "following:\n",
    "\n",
    "    a. Implement the sigmoid function from scratch and call it sigmoid_f\n",
    "\n",
    "    b. Implement the hypothesis function from scratch and call it classifier_f\n",
    "\n",
    "    c. Implement the entropy function as your cost function and call it binary_loss_f\n",
    "\n",
    "    d. Implement gradient descent for logistic regression and call it gradient_f\n",
    "\n",
    "    e. Combining the functionalities of what you have coded above, create an optimizer\n",
    "    function and call it optimizer_f. \n",
    "\n",
    "Note: You should find out the input and\n",
    "output to the functions above by reviewing the class notes and the textbook; in\n",
    "other words, this will be part of the challenge! If needed, use 265 as your random\n",
    "seed.\n",
    "\n",
    "Letâ€™s test your code on a dataset. Load the Breast Cancer Wisconsin Dataset provided\n",
    "by sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\n",
    "Now, do the following:\n",
    "\n",
    "    a. Set the target column as your Y variable.\n",
    "\n",
    "    b. Set all other numeric variables (excluding index) as your X matrix.\n",
    "\n",
    "    c. Apply 0-1 normalization on both the X matrix and Y vector.\n",
    "\n",
    "    d. Run logistic regression by using the code you have written (no need to do\n",
    "    train/test split). Set the maximum number of iterations to 10,000.\n",
    "\n",
    "    e. Report the final equation you have obtained for logistic regression.\n",
    "\n",
    "    f. Also indicate which coefficients are positively associated and which\n",
    "    coefficients are negatively associated with the target variable. Rank them\n",
    "    from positive to negative. Interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9f61e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid_f(z):\n",
    "    s_val = 1/(1 + math.e**(-z)) # returns the result of the sigmoid funtion\n",
    "    return (s_val)\n",
    "\n",
    "def classifier_f(X, weights):\n",
    "    z = np.dot(X, weights) # calculates predicted values for dependent variable\n",
    "    return sigmoid_f(z)\n",
    "\n",
    "def binary_loss_f(y_pred, y):                 \n",
    "    return sum(-y*np.log(y_pred) - (1-y)*np.log(1-y_pred))/len(y) # returns binary loss value\n",
    "\n",
    "def gradient_f(X, weights, learning_rate, num_iters):\n",
    "    weights = np.random.random(X.shape[1])\n",
    "    costs = []\n",
    "    for epoch in range(num_iters):\n",
    "        y_pred = classifier_f(X, weights) # predicted values \n",
    "        loss = y_pred - y \n",
    "        weights_grad = X.T.dot(loss)\n",
    "        weights -= learning_rate * weights_grad # updates the weight vector\n",
    "        cost = binary_loss_f(y_pred, y) # calculates cost\n",
    "        costs.append(cost) # saves the cost history\n",
    "        if (epoch%100==0):\n",
    "            print(f\"Epoch :{epoch} || Cost: {float(cost)}\")    \n",
    "    return weights, costs\n",
    "\n",
    "def optimizer_f(X, y, learning_rate, num_iters):\n",
    "    weights, cost = gradient_f(X, y, learning_rate, num_iters) # returns weight and cost vectors\n",
    "    print(\"weights = \", weights)\n",
    "    plt.plot(cost)\n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d546ef55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# load dataset\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee31505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "957c7452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "X = Scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e93ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0 || Cost: 1.5250591192914391\n",
      "Epoch :100 || Cost: 0.230228010005412\n",
      "Epoch :200 || Cost: 0.18737108257039845\n",
      "Epoch :300 || Cost: 0.16545774264825774\n",
      "Epoch :400 || Cost: 0.1518439668600528\n",
      "Epoch :500 || Cost: 0.1424101186720937\n",
      "Epoch :600 || Cost: 0.1353939701150019\n",
      "Epoch :700 || Cost: 0.1299137397319381\n",
      "Epoch :800 || Cost: 0.1254774935286902\n",
      "Epoch :900 || Cost: 0.12178785796530608\n",
      "Epoch :1000 || Cost: 0.11865369261299923\n",
      "Epoch :1100 || Cost: 0.1159460656655422\n",
      "Epoch :1200 || Cost: 0.11357456363292885\n",
      "Epoch :1300 || Cost: 0.11147372738668787\n",
      "Epoch :1400 || Cost: 0.1095948781441754\n",
      "Epoch :1500 || Cost: 0.10790097606723655\n",
      "Epoch :1600 || Cost: 0.10636326759170042\n",
      "Epoch :1700 || Cost: 0.10495903170656992\n",
      "Epoch :1800 || Cost: 0.10367002579590036\n",
      "Epoch :1900 || Cost: 0.10248139070443248\n",
      "Epoch :2000 || Cost: 0.10138086521559592\n",
      "Epoch :2100 || Cost: 0.10035821350971302\n",
      "Epoch :2200 || Cost: 0.09940480174774173\n",
      "Epoch :2300 || Cost: 0.09851328050417289\n",
      "Epoch :2400 || Cost: 0.09767734320568809\n",
      "Epoch :2500 || Cost: 0.0968915397496337\n",
      "Epoch :2600 || Cost: 0.09615113065227944\n",
      "Epoch :2700 || Cost: 0.09545197135486314\n",
      "Epoch :2800 || Cost: 0.09479041929202543\n",
      "Epoch :2900 || Cost: 0.09416325839998958\n",
      "Epoch :3000 || Cost: 0.0935676371862991\n",
      "Epoch :3100 || Cost: 0.09300101749298484\n",
      "Epoch :3200 || Cost: 0.09246113179688345\n",
      "Epoch :3300 || Cost: 0.0919459473988434\n",
      "Epoch :3400 || Cost: 0.0914536362220907\n",
      "Epoch :3500 || Cost: 0.09098254921241285\n",
      "Epoch :3600 || Cost: 0.09053119453797821\n",
      "Epoch :3700 || Cost: 0.09009821894387476\n",
      "Epoch :3800 || Cost: 0.08968239173885709\n",
      "Epoch :3900 || Cost: 0.08928259098826333\n",
      "Epoch :4000 || Cost: 0.08889779156385214\n",
      "Epoch :4100 || Cost: 0.08852705476292833\n",
      "Epoch :4200 || Cost: 0.08816951925886296\n",
      "Epoch :4300 || Cost: 0.08782439318546412\n",
      "Epoch :4400 || Cost: 0.08749094719055595\n",
      "Epoch :4500 || Cost: 0.08716850832101268\n",
      "Epoch :4600 || Cost: 0.08685645462358967\n",
      "Epoch :4700 || Cost: 0.08655421036408326\n",
      "Epoch :4800 || Cost: 0.08626124178239497\n",
      "Epoch :4900 || Cost: 0.08597705331354501\n",
      "Epoch :5000 || Cost: 0.08570118421506778\n",
      "Epoch :5100 || Cost: 0.08543320554988547\n",
      "Epoch :5200 || Cost: 0.08517271748102537\n",
      "Epoch :5300 || Cost: 0.08491934684064997\n",
      "Epoch :5400 || Cost: 0.08467274494102328\n",
      "Epoch :5500 || Cost: 0.08443258559939826\n",
      "Epoch :5600 || Cost: 0.08419856335250848\n",
      "Epoch :5700 || Cost: 0.08397039183950787\n",
      "Epoch :5800 || Cost: 0.08374780233489058\n",
      "Epoch :5900 || Cost: 0.0835305424152382\n",
      "Epoch :6000 || Cost: 0.0833183747456214\n",
      "Epoch :6100 || Cost: 0.08311107597320025\n",
      "Epoch :6200 || Cost: 0.08290843571704098\n",
      "Epoch :6300 || Cost: 0.08271025564445245\n",
      "Epoch :6400 || Cost: 0.08251634862525672\n",
      "Epoch :6500 || Cost: 0.08232653795638176\n",
      "Epoch :6600 || Cost: 0.08214065665000639\n",
      "Epoch :6700 || Cost: 0.08195854677923345\n",
      "Epoch :6800 || Cost: 0.08178005887591262\n",
      "Epoch :6900 || Cost: 0.08160505137581052\n",
      "Epoch :7000 || Cost: 0.08143339010682646\n",
      "Epoch :7100 || Cost: 0.08126494781639315\n",
      "Epoch :7200 || Cost: 0.08109960373460137\n",
      "Epoch :7300 || Cost: 0.08093724316993021\n",
      "Epoch :7400 || Cost: 0.08077775713477284\n",
      "Epoch :7500 || Cost: 0.08062104199822716\n",
      "Epoch :7600 || Cost: 0.08046699916386073\n",
      "Epoch :7700 || Cost: 0.08031553477037595\n",
      "Epoch :7800 || Cost: 0.0801665594133038\n",
      "Epoch :7900 || Cost: 0.08001998788601984\n",
      "Epoch :8000 || Cost: 0.07987573893853792\n",
      "Epoch :8100 || Cost: 0.07973373505267486\n",
      "Epoch :8200 || Cost: 0.07959390223230796\n",
      "Epoch :8300 || Cost: 0.07945616980755486\n",
      "Epoch :8400 || Cost: 0.07932047025181511\n",
      "Epoch :8500 || Cost: 0.07918673901069848\n",
      "Epoch :8600 || Cost: 0.07905491434195017\n",
      "Epoch :8700 || Cost: 0.0789249371655625\n",
      "Epoch :8800 || Cost: 0.07879675092332199\n",
      "Epoch :8900 || Cost: 0.07867030144711228\n",
      "Epoch :9000 || Cost: 0.07854553683534181\n",
      "Epoch :9100 || Cost: 0.07842240733692085\n",
      "Epoch :9200 || Cost: 0.07830086524225674\n",
      "Epoch :9300 || Cost: 0.07818086478077768\n",
      "Epoch :9400 || Cost: 0.07806236202453698\n",
      "Epoch :9500 || Cost: 0.07794531479747985\n",
      "Epoch :9600 || Cost: 0.07782968258999315\n",
      "Epoch :9700 || Cost: 0.0777154264783811\n",
      "Epoch :9800 || Cost: 0.07760250904894211\n",
      "Epoch :9900 || Cost: 0.07749089432634176\n",
      "weights =  [ 21.83938371   1.59679199  17.79131246  -9.84279612   6.81421289\n",
      "  -3.51228902 -16.36068281 -28.8908349    8.90161143  21.48705721\n",
      " -16.73535483   7.4374776   -9.6636914  -20.12650306  -3.2161601\n",
      "  -0.16602327   7.4048269    3.71846032   9.01465592   3.58531003\n",
      "  -2.63448381  -9.6823236    1.2701837  -23.4098471   -0.11542564\n",
      "   1.19253629   2.49233147  -0.43311392 -14.54950763  -9.02942077]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set learning rate and epochs\n",
    "\n",
    "number_of_iterations = 10000\n",
    "learning_rate = 0.01\n",
    "weights_opt = optimizer_f(X, y, learning_rate = learning_rate,num_iters = number_of_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5a4126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.839383705191462*mean radius +(1.596791990260688)*mean texture +(17.791312463824244)*mean perimeter +(-9.842796122741028)*mean area +(6.814212888124369)*mean smoothness +(-3.5122890164092)*mean compactness +(-16.36068280638367)*mean concavity +(-28.89083490172692)*mean concave points +(8.901611425176887)*mean symmetry +(21.48705720808176)*mean fractal dimension +(-16.73535483059294)*radius error +(7.437477599025896)*texture error +(-9.663691395411341)*perimeter error +(-20.126503059831077)*area error +(-3.216160100235993)*smoothness error +(-0.16602327329112487)*compactness error +(7.404826899700334)*concavity error +(3.718460317010289)*concave points error +(9.014655920673821)*symmetry error +(3.585310030168413)*fractal dimension error +(-2.634483814563519)*worst radius +(-9.682323599867244)*worst texture +(1.2701837043219981)*worst perimeter +(-23.409847103325067)*worst area +(-0.11542563576074742)*worst smoothness +(1.19253628743609)*worst compactness +(2.4923314655389976)*worst concavity +(-0.4331139163786498)*worst concave points +(-14.549507629636972)*worst symmetry +(-9.029420769188915)*worst fractal dimension \n"
     ]
    }
   ],
   "source": [
    "# print log reg equation\n",
    "\n",
    "eq = \"\"\n",
    "for i, col in enumerate(breast_cancer.feature_names):\n",
    "    if i == 0:\n",
    "        eq += f\"{weights_opt[i]}*{col}\"\n",
    "        eq += \" \"\n",
    "    else:\n",
    "        eq += f\"+({weights_opt[i]})*{col}\"\n",
    "        eq += \" \"\n",
    "print(eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c88ec309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive cofficient features : [('mean radius', 21.839383705191462), ('mean fractal dimension', 21.48705720808176), ('mean perimeter', 17.791312463824244), ('symmetry error', 9.014655920673821), ('mean symmetry', 8.901611425176887), ('texture error', 7.437477599025896), ('concavity error', 7.404826899700334), ('mean smoothness', 6.814212888124369), ('concave points error', 3.718460317010289), ('fractal dimension error', 3.585310030168413), ('worst concavity', 2.4923314655389976), ('mean texture', 1.596791990260688), ('worst perimeter', 1.2701837043219981), ('worst compactness', 1.19253628743609)]\n",
      "Negative cofficient features : [('worst smoothness', -0.11542563576074742), ('compactness error', -0.16602327329112487), ('worst concave points', -0.4331139163786498), ('worst radius', -2.634483814563519), ('smoothness error', -3.216160100235993), ('mean compactness', -3.5122890164092), ('worst fractal dimension', -9.029420769188915), ('perimeter error', -9.663691395411341), ('worst texture', -9.682323599867244), ('mean area', -9.842796122741028), ('worst symmetry', -14.549507629636972), ('mean concavity', -16.36068280638367), ('radius error', -16.73535483059294), ('area error', -20.126503059831077), ('worst area', -23.409847103325067), ('mean concave points', -28.89083490172692)]\n"
     ]
    }
   ],
   "source": [
    "# separate negative and positive coefficients and respective attributes into separate dictionaries\n",
    "\n",
    "coeffs_pos = {}\n",
    "coeffs_neg = {}\n",
    "for i, col in enumerate(breast_cancer.feature_names):\n",
    "    if weights_opt[i]>0:\n",
    "        coeffs_pos[col] = weights_opt[i]\n",
    "    else:\n",
    "        coeffs_neg[col] = weights_opt[i]\n",
    "print(\"Attributes with positive cofficients:\", sorted(coeffs_pos.items(), key=lambda x: x[1],reverse=True))\n",
    "print(\"Attributes with negative cofficients:\", sorted(coeffs_neg.items(), key=lambda x: x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaecbec",
   "metadata": {},
   "source": [
    "References:\n",
    "https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363c7cb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b029266",
   "metadata": {},
   "source": [
    "## Question 3:\n",
    "\n",
    "Implement the three following cross-validation algorithms from scratch:\n",
    "\n",
    "a. Leave-one-out cross-validation\n",
    "\n",
    "b. K-fold cross-validation\n",
    "\n",
    "c. Train-test split cross-validation\n",
    "\n",
    "Test your results on the California Housing Dataset:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing\n",
    "\n",
    "Now, do the following:\n",
    "\n",
    "    a. Implement the cross-validation algorithms from scratch.\n",
    "\n",
    "    b. Choose the following features from the dataset as your X matrix: MedInc,\n",
    "    HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude\n",
    "\n",
    "    c. Choose the following feature from the dataset as your Y matrix: MedHouseVal\n",
    "\n",
    "    d. Apply 0 â€“ 1 normalization on X and Y.\n",
    "\n",
    "    e. Apply the cross-validation algorithms that you implemented to train your model.\n",
    "\n",
    "    (For splitting your data always use 265 as your random number or seed value).\n",
    "    \n",
    "   Note: You cannot use the pre-packaged algorithms for splitting the data. \n",
    "\n",
    "   To split the data please do the following:\n",
    "\n",
    "        a. Install the random package written for Python\n",
    "\n",
    "        b. Set (the initial) random.seed() to 265\n",
    "\n",
    "        c. Create a list of integers that will function as your index numbers:\n",
    "        list(range(0,len(name_of_dataset))\n",
    "\n",
    "        d. Pick one integer for Train-Test Split CV from the list your created in c) to\n",
    "        split 70% of your data as training set and the remaining 30% as the test set.\n",
    "\n",
    "        e. For the K-fold CV, set k = 5. Please divide the dataset into 5 quasi-equal\n",
    "        portions starting from index 0.\n",
    "\n",
    "        f. For LOOCV, start the training by randomly picking a feature vector\n",
    "        associated with an index in your dataset (Reminder: random seed is 265)\n",
    "        â€“ you will need to run the model on every point.\n",
    "\n",
    "    f. Using scikitâ€™s sklearn.linear_model.LinearRegression, predict\n",
    "    the house prices by using all of the data in your X matrix. Compare different\n",
    "    techniques of CV. Which CV provides the lowest MSE? Why? Interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93523e74",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "55a78772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error calculation\n",
    "def MSE(y, y_pred):\n",
    "    return np.square(np.subtract(y, y_pred)).mean()\n",
    "\n",
    "# leave one out cross validation\n",
    "def leave_one_out_CV(X, y, model_name):\n",
    "    n = len(X)\n",
    "    errors = []\n",
    "    models_dict = {}\n",
    "\n",
    "    for i in range(n):\n",
    "        X_train, X_test = X.drop(i,axis=0), X.iloc[i,:].values\n",
    "        y_train, y_test = np.delete(y,i), y[i]\n",
    "        model = model_name\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test.reshape(1,-1))\n",
    "        MSEscore = MSE(y_test, y_pred)\n",
    "        if idx%1000==0:\n",
    "            print(f\"MSE score for model- {i} is \", MSEscore)\n",
    "            errors.append(MSEscore)\n",
    "            models_dict[i] = model\n",
    "            \n",
    "    print(\"Average mean of MSE is\", np.mean(errors))\n",
    "    return models_dict, errors\n",
    "\n",
    "# k fold cross validation basic code\n",
    "def k_fold_CV(dataset, k):\n",
    "    datasplit = {}\n",
    "    num_ele_per_fold = int(dataset.shape[0]/k)\n",
    "    for i in range(k):\n",
    "        fold=[]\n",
    "        while (len(fold) < num_ele_per_fold):\n",
    "            r = randrange(dataset.shape[0])\n",
    "            ind = dataset.index[r]\n",
    "            fold.append(ind)\n",
    "            dataset = dataset.drop(ind)\n",
    "        datasplit[i]=np.asarray(fold)\n",
    "    return datasplit\n",
    "\n",
    "# implementation of k fold cross validation\n",
    "def k_fold_CV_imp(X, y, k, model_name):\n",
    "        errors = []\n",
    "        models_dict = {}\n",
    "        fold_ids = k_fold_CV(X, k)\n",
    "        for ind in range(k):\n",
    "            print(f\"{ind} fold\")\n",
    "            X_train, X_test = X.drop(fold_ids[ind],axis=0), X.iloc[fold_ids[ind],:].values\n",
    "            y_train, y_test = np.delete(y, fold_ids[ind]),y[fold_ids[ind]]\n",
    "            model = model_name\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            MSEscore = MSE(y_test, y_pred)\n",
    "            print(f\"MSE score for model {ind} is \", MSEscore)\n",
    "            print(\"-------------------------------------------------------------------\")\n",
    "            errors.append(MSEscore)\n",
    "            models_dict[ind] = model\n",
    "        print(\"-------------------------------------------------------------------\")\n",
    "        print(\"Average mean of the MSE error is\", np.mean(errors))\n",
    "        return models_dict, errors\n",
    "\n",
    "# train test split\n",
    "def train_test_split(X, y, model_name): \n",
    "            df = pd.concat([X, pd.DataFrame(y)], axis=1)\n",
    "            df = df.sample(frac=1, random_state=265)\n",
    "            indices = range(X.shape[0])\n",
    "            train_size = int(0.7 * X.shape[0])\n",
    "            train_indices = indices[:train_size]\n",
    "            test_indices = indices[train_size:]\n",
    "            X_train, X_test = df.iloc[train_indices],df.iloc[test_indices]\n",
    "            y_train, y_test = df.iloc[train_indices,-1],df.iloc[test_indices,-1]\n",
    "            model = model_name\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            MSEscore = MSE(y_test, y_pred)\n",
    "            print(f\"MSE score for model is \", MSEscore)\n",
    "            return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d0573b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold\n",
      "MSE score for model 0 is  0.529974940061078\n",
      "-------------------------------------------------------------------\n",
      "1 fold\n",
      "MSE score for model 1 is  0.5477645459252342\n",
      "-------------------------------------------------------------------\n",
      "2 fold\n",
      "MSE score for model 2 is  0.5040908861674307\n",
      "-------------------------------------------------------------------\n",
      "3 fold\n",
      "MSE score for model 3 is  0.5278838122736698\n",
      "-------------------------------------------------------------------\n",
      "4 fold\n",
      "MSE score for model 4 is  0.5391523948478724\n",
      "-------------------------------------------------------------------\n",
      "-------------------------------------------------------------------\n",
      "Average mean of the MSE error is 0.529773315855057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "from random import randrange\n",
    "\n",
    "np.random.seed(265)\n",
    "random.seed(265)\n",
    "\n",
    "# importing the data\n",
    "df_cal_housing = fetch_california_housing() \n",
    "\n",
    "X = df_cal_housing.data\n",
    "y = df_cal_housing.target\n",
    "\n",
    "# scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns = df_cal_housing.feature_names)\n",
    "\n",
    "# performing k fold cross validation on the data\n",
    "models, errors = k_fold_CV_imp(X, y, 5, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c19f89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE score for model is  9.032338394368907e-31\n"
     ]
    }
   ],
   "source": [
    "# performing train test split on the data\n",
    "model = train_test_split(X, y, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ef308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
