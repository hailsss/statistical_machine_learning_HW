{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835c1c10",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f423e27",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b653ae",
   "metadata": {},
   "source": [
    "For (most of) the questions below, please use the fake news dataset uploaded on BlackBoard\n",
    "(called ‘corona_fake.csv’). You can find the file under ‘Data’ tab.\n",
    "Please include your code also in your .pdf file (in code blocks).\n",
    "\n",
    "Data Pre-Processing (40 points)\n",
    "\n",
    "[20 points] Using the pandas package for Python, import the corona_fake.csv dataset, and do the following:\n",
    "\n",
    "    a) [5 points] Import the nltk package. Check the documentation:\n",
    "    https://www.nltk.org/\n",
    "\n",
    "    b) [15 points] Take a look at the text column in the dataset, and do the following:\n",
    "\n",
    "        i. [3 points] Using nltk.word_tokenize(), tokenize the text.\n",
    "\n",
    "        ii. [3 points] Using the POS-tagging feature (nltk.pos_tag), POS-tag the\n",
    "        tokenized words.\n",
    "\n",
    "        iii. [3 points] Using WordNetLemmatizer (from nltk.stem import\n",
    "        WordNetLemmatizer) lemmatize the pos-tagged words you obtained\n",
    "        above. (Hint: If there is no available tag, append the token as is; else, use the\n",
    "        tag to lemmatize the token)\n",
    "\n",
    "        iv. [3 points] Using the list of stop words that can be imported (nltk.corpus\n",
    "        import stopwords), remove the stopwords in lemmatized text [Note: the\n",
    "        language needs to be set as ‘english’.].\n",
    "\n",
    "        v. [3 points] Finally, also remove numbers, words that are shorter than 2\n",
    "        characters, punctuation, links and emojis. Finally, convert the obtained list of\n",
    "        tokenized+tagged+lemmatized+cleaned list of words back into a joined string\n",
    "        (joined by space ‘ ‘ ) and add the result as text_clean column to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce14e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/haileythanki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the required libraries\n",
    "\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1922a259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Could the Power of the Sun Slow the Coronavirus?</td>\n",
       "      <td>A study suggests that ultraviolet rays could s...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Key evidence for coronavirus spread is flawed ...</td>\n",
       "      <td>Last week, a medical journal reported that a b...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Summer Heat May Not Diminish Coronavirus Strength</td>\n",
       "      <td>A new report, sent to the White House science ...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>How Long Will a Vaccine Really Take?</td>\n",
       "      <td>A vaccine would be the ultimate weapon against...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Why Funding the Covid-19 Response Could Be the...</td>\n",
       "      <td>Developing and delivering coronavirus vaccines...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1159 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1154   Could the Power of the Sun Slow the Coronavirus?   \n",
       "1155  Key evidence for coronavirus spread is flawed ...   \n",
       "1156  Summer Heat May Not Diminish Coronavirus Strength   \n",
       "1157               How Long Will a Vaccine Really Take?   \n",
       "1158  Why Funding the Covid-19 Response Could Be the...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     You just need to add water, and the drugs and ...   \n",
       "1     Hydroxychloroquine has been shown to have a 10...   \n",
       "2     Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3     The Corona virus is a man made virus created i...   \n",
       "4     Doesn’t @BillGates finance research at the Wuh...   \n",
       "...                                                 ...   \n",
       "1154  A study suggests that ultraviolet rays could s...   \n",
       "1155  Last week, a medical journal reported that a b...   \n",
       "1156  A new report, sent to the White House science ...   \n",
       "1157  A vaccine would be the ultimate weapon against...   \n",
       "1158  Developing and delivering coronavirus vaccines...   \n",
       "\n",
       "                         source label  \n",
       "0     coronavirusmedicalkit.com  fake  \n",
       "1                  RudyGiuliani  fake  \n",
       "2                   CharlieKirk  fake  \n",
       "3       JoanneWrightForCongress  fake  \n",
       "4       JoanneWrightForCongress  fake  \n",
       "...                         ...   ...  \n",
       "1154   https://www.nytimes.com/  true  \n",
       "1155   https://www.nytimes.com/  true  \n",
       "1156   https://www.nytimes.com/  true  \n",
       "1157   https://www.nytimes.com/  true  \n",
       "1158   https://www.nytimes.com/  true  \n",
       "\n",
       "[1159 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "\n",
    "df_coronaFake = pd.read_csv(\"/Users/haileythanki/Downloads/corona_fake.csv\")\n",
    "df_coronaFake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf8539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'just',\n",
       " 'need',\n",
       " 'to',\n",
       " 'add',\n",
       " 'water',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'drugs',\n",
       " 'and',\n",
       " 'vaccines',\n",
       " 'are',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'be',\n",
       " 'administered',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'two',\n",
       " 'parts',\n",
       " 'to',\n",
       " 'the',\n",
       " 'kit',\n",
       " ':',\n",
       " 'one',\n",
       " 'holds',\n",
       " 'pellets',\n",
       " 'containing',\n",
       " 'the',\n",
       " 'chemical',\n",
       " 'machinery',\n",
       " 'that',\n",
       " 'synthesises',\n",
       " 'the',\n",
       " 'end',\n",
       " 'product',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'other',\n",
       " 'holds',\n",
       " 'pellets',\n",
       " 'containing',\n",
       " 'instructions',\n",
       " 'that',\n",
       " 'telll',\n",
       " 'the',\n",
       " 'drug',\n",
       " 'which',\n",
       " 'compound',\n",
       " 'to',\n",
       " 'create',\n",
       " '.',\n",
       " 'Mix',\n",
       " 'two',\n",
       " 'parts',\n",
       " 'together',\n",
       " 'in',\n",
       " 'a',\n",
       " 'chosen',\n",
       " 'combination',\n",
       " ',',\n",
       " 'add',\n",
       " 'water',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'treatment',\n",
       " 'is',\n",
       " 'ready',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function definition to identify empty strings\n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "\n",
    "# tokenize the strings in the text column and store them in an array\n",
    "\n",
    "text_tokenized_arr = []\n",
    "\n",
    "for i in range(0, 1159):\n",
    "    if (isNaN(df_coronaFake['text'][i])):\n",
    "        text_tokenized_arr.append(np.nan)\n",
    "    else:\n",
    "        text_tokenized = nltk.word_tokenize(df_coronaFake['text'][i])\n",
    "        text_tokenized_arr.append(text_tokenized)\n",
    "        \n",
    "# print the first tokenized string\n",
    "\n",
    "text_tokenized_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdf1c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('need', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('add', 'VB'),\n",
       " ('water', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('drugs', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('vaccines', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('ready', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('administered', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('There', 'EX'),\n",
       " ('are', 'VBP'),\n",
       " ('two', 'CD'),\n",
       " ('parts', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('kit', 'NN'),\n",
       " (':', ':'),\n",
       " ('one', 'CD'),\n",
       " ('holds', 'VBZ'),\n",
       " ('pellets', 'NNS'),\n",
       " ('containing', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('chemical', 'NN'),\n",
       " ('machinery', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('synthesises', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('end', 'NN'),\n",
       " ('product', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('holds', 'VBZ'),\n",
       " ('pellets', 'NNS'),\n",
       " ('containing', 'VBG'),\n",
       " ('instructions', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('telll', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('drug', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('compound', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('create', 'VB'),\n",
       " ('.', '.'),\n",
       " ('Mix', 'NNP'),\n",
       " ('two', 'CD'),\n",
       " ('parts', 'NNS'),\n",
       " ('together', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('chosen', 'NN'),\n",
       " ('combination', 'NN'),\n",
       " (',', ','),\n",
       " ('add', 'JJ'),\n",
       " ('water', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('treatment', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('ready', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tag the tokenized strings and store them in an array\n",
    "\n",
    "text_pos_tagged_arr = []\n",
    "\n",
    "for i in range(0, 1159):\n",
    "    if (isNaN(df_coronaFake['text'][i])):\n",
    "        text_pos_tagged_arr.append(np.nan)\n",
    "\n",
    "    else:\n",
    "        text = df_coronaFake['text'][i]\n",
    "        text_tokenized = nltk.word_tokenize(text)\n",
    "        text_tokenized = [word for word in text_tokenized]   \n",
    "        text_pos_tagged = nltk.pos_tag(text_tokenized)\n",
    "        text_pos_tagged_arr.append(text_pos_tagged)\n",
    "\n",
    "# print the first tokenized and POS tagged string\n",
    "\n",
    "text_pos_tagged_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2903046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You just need to add water , and the drug and vaccine be ready to be administer . There be two part to the kit : one hold pellet contain the chemical machinery that synthesise the end product , and the other hold pellet contain instruction that telll the drug which compound to create . Mix two part together in a chosen combination , add water , and the treatment be ready .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function definition for converting the POS tagged strings to nltk-friendly POS tags\n",
    "\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# function definition for lemmatizing POS tagged strings\n",
    "\n",
    "def lemmatize_sentence(text):\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    text_wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    text_lemmatized = []\n",
    "    \n",
    "    for word, tag in text_wordnet_tagged:\n",
    "        if tag is None:\n",
    "            text_lemmatized.append(word)\n",
    "        else:        \n",
    "            text_lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(text_lemmatized)\n",
    "\n",
    "text_lemmatized_arr = []\n",
    "\n",
    "# calling the lemmatizer function to convert all strings in the text column to lemmatized strings\n",
    "\n",
    "for i in range(0, 1159):\n",
    "    if (isNaN(df_coronaFake['text'][i])):\n",
    "        text_lemmatized_arr.append(np.nan)\n",
    "    else:\n",
    "        text = df_coronaFake['text'][i]\n",
    "        text_lemmatized = lemmatize_sentence(text)\n",
    "        text_lemmatized_arr.append(text_lemmatized)\n",
    "    \n",
    "# printing the first lemmatized string\n",
    "\n",
    "text_lemmatized_arr[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b22eb",
   "metadata": {},
   "source": [
    "Reference: https://www.holisticseo.digital/python-seo/nltk/lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2488804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need add water , drug vaccine ready administer . There two part kit : one hold pellet contain chemical machinery synthesise end product , hold pellet contain instruction telll drug compound create . Mix two part together chosen combination , add water , treatment ready .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text_stopwords_rem_arr = []\n",
    "\n",
    "# function definition for lemmatizing sentences and removing stop words at the same time\n",
    "\n",
    "def lemmatize_sentence_rem_stop_words(title):\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    text_wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    text_lemmatized = []\n",
    "    \n",
    "    for word, tag in text_wordnet_tagged:\n",
    "            if tag is None and word not in stop_words :\n",
    "                text_lemmatized.append(word)\n",
    "            elif word not in stop_words:        \n",
    "                text_lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(text_lemmatized)\n",
    "\n",
    "text_lemmatized_arr = []\n",
    "\n",
    "# calling the function for lemmatization and removing stop words\n",
    "\n",
    "for i in range(0, 1159):\n",
    "    if (isNaN(df_coronaFake['text'][i])):\n",
    "        text_stopwords_rem_arr.append(np.nan)\n",
    "    else:\n",
    "        text = df_coronaFake['text'][i]\n",
    "        text_stopwords_rem = lemmatize_sentence_rem_stop_words(text)\n",
    "        text_stopwords_rem_arr.append(text_stopwords_rem)\n",
    "    \n",
    "text_stopwords_rem_arr[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722e18e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You need add water drug vaccine ready administer There two part kit one hold pellet contain chemical machinery synthesise end product hold pellet contain instruction telll drug compound create Mix two part together chosen combination add water treatment ready'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove numbers, words that are shorter than 2 characters, punctuation, links and emojis \n",
    "\n",
    "import re\n",
    "\n",
    "text_clean_arr = []\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F]+\")\n",
    "\n",
    "# remove punctuation, numbers and words that are shorter than 2 characters\n",
    "for text in text_stopwords_rem_arr:\n",
    "    if (isNaN(text)):\n",
    "        text_clean_arr.append(np.nan)\n",
    "    else:\n",
    "        \n",
    "        text_split = text.split(\" \")\n",
    "        text_clean = []\n",
    "        for word in text_split:\n",
    "            if (len(word)>=2 and not(word.replace('.','',1).isnumeric()) and not(word.replace(',','',1).isnumeric())):\n",
    "                text_clean.append(word)\n",
    "        text_clean = (\" \".join(text_clean))\n",
    "        text_clean = emoji_pattern.sub(r'', text_clean) # remove emojis\n",
    "        text_clean = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text_clean, flags=re.MULTILINE) # remove links\n",
    "        text_clean_arr.append(text_clean)\n",
    "        \n",
    "text_clean_arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c073b0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8719d979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>You need add water drug vaccine ready administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>fake</td>\n",
       "      <td>Hydroxychloroquine show effective rate treat C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fact Hydroxychloroquine show effective rate tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "      <td>The Corona virus man make virus create Wuhan l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "      <td>Doesn BillGates finance research Wuhan lab Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Could the Power of the Sun Slow the Coronavirus?</td>\n",
       "      <td>A study suggests that ultraviolet rays could s...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>study suggest ultraviolet ray could slow virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Key evidence for coronavirus spread is flawed ...</td>\n",
       "      <td>Last week, a medical journal reported that a b...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>Last week medical journal report business trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Summer Heat May Not Diminish Coronavirus Strength</td>\n",
       "      <td>A new report, sent to the White House science ...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>new report send White House science adviser sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>How Long Will a Vaccine Really Take?</td>\n",
       "      <td>A vaccine would be the ultimate weapon against...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>vaccine would ultimate weapon coronavirus best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Why Funding the Covid-19 Response Could Be the...</td>\n",
       "      <td>Developing and delivering coronavirus vaccines...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>Developing deliver coronavirus vaccine test tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1159 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1154   Could the Power of the Sun Slow the Coronavirus?   \n",
       "1155  Key evidence for coronavirus spread is flawed ...   \n",
       "1156  Summer Heat May Not Diminish Coronavirus Strength   \n",
       "1157               How Long Will a Vaccine Really Take?   \n",
       "1158  Why Funding the Covid-19 Response Could Be the...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     You just need to add water, and the drugs and ...   \n",
       "1     Hydroxychloroquine has been shown to have a 10...   \n",
       "2     Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3     The Corona virus is a man made virus created i...   \n",
       "4     Doesn’t @BillGates finance research at the Wuh...   \n",
       "...                                                 ...   \n",
       "1154  A study suggests that ultraviolet rays could s...   \n",
       "1155  Last week, a medical journal reported that a b...   \n",
       "1156  A new report, sent to the White House science ...   \n",
       "1157  A vaccine would be the ultimate weapon against...   \n",
       "1158  Developing and delivering coronavirus vaccines...   \n",
       "\n",
       "                         source label  \\\n",
       "0     coronavirusmedicalkit.com  fake   \n",
       "1                  RudyGiuliani  fake   \n",
       "2                   CharlieKirk  fake   \n",
       "3       JoanneWrightForCongress  fake   \n",
       "4       JoanneWrightForCongress  fake   \n",
       "...                         ...   ...   \n",
       "1154   https://www.nytimes.com/  true   \n",
       "1155   https://www.nytimes.com/  true   \n",
       "1156   https://www.nytimes.com/  true   \n",
       "1157   https://www.nytimes.com/  true   \n",
       "1158   https://www.nytimes.com/  true   \n",
       "\n",
       "                                             text_clean  \n",
       "0     You need add water drug vaccine ready administ...  \n",
       "1     Hydroxychloroquine show effective rate treat C...  \n",
       "2     Fact Hydroxychloroquine show effective rate tr...  \n",
       "3     The Corona virus man make virus create Wuhan l...  \n",
       "4     Doesn BillGates finance research Wuhan lab Cor...  \n",
       "...                                                 ...  \n",
       "1154  study suggest ultraviolet ray could slow virus...  \n",
       "1155  Last week medical journal report business trav...  \n",
       "1156  new report send White House science adviser sa...  \n",
       "1157  vaccine would ultimate weapon coronavirus best...  \n",
       "1158  Developing deliver coronavirus vaccine test tr...  \n",
       "\n",
       "[1159 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe with the text_clean column\n",
    "\n",
    "df_coronaFake[\"text_clean\"] = text_clean_arr\n",
    "df_coronaFake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99681b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b23be92",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f18d6",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6281447",
   "metadata": {},
   "source": [
    "\n",
    "[20 points] Let’s vectorize the data we produced above by using two approaches: Bag of\n",
    "Words (BOW) and TF-IDF; and, at the end, we will make a prediction:\n",
    "\n",
    "    a. [5 points] Read the following page: https://en.wikipedia.org/wiki/N-gram. Explain\n",
    "    what an ‘n-gram’ is and why it is helpful in max. 200 words.\n",
    "    \n",
    "    b. [5 points] Import CountVectorizer and TfidfVectorizer:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "    \n",
    "    c. [5 points] Using CountVectorizer, create three vectorized representations of\n",
    "    text_clean [set lowercase=True]:\n",
    "    \n",
    "        i. One vectorized representation where ngram_range = (1,1)\n",
    "        ii. One vectorized representation where ngram_range = (1,2)\n",
    "        iii. One vectorized representation where ngram_range = (1,3)\n",
    "        \n",
    "    d. [5 points] Using TfidfVectorizer, create three vectorized representations of\n",
    "    text_clean [set lowercase=True]:\n",
    "    \n",
    "        i. One vectorized representation where ngram_range = (1,1)\n",
    "        ii. One vectorized representation where ngram_range = (1,2)\n",
    "        iii. One vectorized representation where ngram_range = (1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6ff95",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496193f",
   "metadata": {},
   "source": [
    "N-gram models are based on Markov models. An N-gram is a sequence of N words where the probability of a word depends on the previous words of the sequence. \n",
    "\n",
    "A simple way to look at it is as follows:\n",
    "\n",
    "$ P(w|H) $: probability of word ‘w’, given some history ‘H’ \n",
    "\n",
    "History refers to the sequence of words before the word for which we are trying to calculate the probability.\n",
    "\n",
    "N-gram models can be used to approximate the probability of a word given all the previous words by using conditional probability of all the preceding words. \n",
    "\n",
    "They are especially useful for speech recognition, machine translation and predictive text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e00ba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>You need add water drug vaccine ready administ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>fake</td>\n",
       "      <td>Hydroxychloroquine show effective rate treat C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>fake</td>\n",
       "      <td>Fact Hydroxychloroquine show effective rate tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "      <td>The Corona virus man make virus create Wuhan l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>fake</td>\n",
       "      <td>Doesn BillGates finance research Wuhan lab Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Could the Power of the Sun Slow the Coronavirus?</td>\n",
       "      <td>A study suggests that ultraviolet rays could s...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>study suggest ultraviolet ray could slow virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Key evidence for coronavirus spread is flawed ...</td>\n",
       "      <td>Last week, a medical journal reported that a b...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>Last week medical journal report business trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Summer Heat May Not Diminish Coronavirus Strength</td>\n",
       "      <td>A new report, sent to the White House science ...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>new report send White House science adviser sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>How Long Will a Vaccine Really Take?</td>\n",
       "      <td>A vaccine would be the ultimate weapon against...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>vaccine would ultimate weapon coronavirus best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Why Funding the Covid-19 Response Could Be the...</td>\n",
       "      <td>Developing and delivering coronavirus vaccines...</td>\n",
       "      <td>https://www.nytimes.com/</td>\n",
       "      <td>true</td>\n",
       "      <td>Developing deliver coronavirus vaccine test tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1154   Could the Power of the Sun Slow the Coronavirus?   \n",
       "1155  Key evidence for coronavirus spread is flawed ...   \n",
       "1156  Summer Heat May Not Diminish Coronavirus Strength   \n",
       "1157               How Long Will a Vaccine Really Take?   \n",
       "1158  Why Funding the Covid-19 Response Could Be the...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     You just need to add water, and the drugs and ...   \n",
       "1     Hydroxychloroquine has been shown to have a 10...   \n",
       "2     Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3     The Corona virus is a man made virus created i...   \n",
       "4     Doesn’t @BillGates finance research at the Wuh...   \n",
       "...                                                 ...   \n",
       "1154  A study suggests that ultraviolet rays could s...   \n",
       "1155  Last week, a medical journal reported that a b...   \n",
       "1156  A new report, sent to the White House science ...   \n",
       "1157  A vaccine would be the ultimate weapon against...   \n",
       "1158  Developing and delivering coronavirus vaccines...   \n",
       "\n",
       "                         source label  \\\n",
       "0     coronavirusmedicalkit.com  fake   \n",
       "1                  RudyGiuliani  fake   \n",
       "2                   CharlieKirk  fake   \n",
       "3       JoanneWrightForCongress  fake   \n",
       "4       JoanneWrightForCongress  fake   \n",
       "...                         ...   ...   \n",
       "1154   https://www.nytimes.com/  true   \n",
       "1155   https://www.nytimes.com/  true   \n",
       "1156   https://www.nytimes.com/  true   \n",
       "1157   https://www.nytimes.com/  true   \n",
       "1158   https://www.nytimes.com/  true   \n",
       "\n",
       "                                             text_clean  \n",
       "0     You need add water drug vaccine ready administ...  \n",
       "1     Hydroxychloroquine show effective rate treat C...  \n",
       "2     Fact Hydroxychloroquine show effective rate tr...  \n",
       "3     The Corona virus man make virus create Wuhan l...  \n",
       "4     Doesn BillGates finance research Wuhan lab Cor...  \n",
       "...                                                 ...  \n",
       "1154  study suggest ultraviolet ray could slow virus...  \n",
       "1155  Last week medical journal report business trav...  \n",
       "1156  new report send White House science adviser sa...  \n",
       "1157  vaccine would ultimate weapon coronavirus best...  \n",
       "1158  Developing deliver coronavirus vaccine test tr...  \n",
       "\n",
       "[1151 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with na values in the test_clean column\n",
    "\n",
    "df_coronaFake = df_coronaFake[df_coronaFake['text_clean'].notna()]\n",
    "df_coronaFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13532cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, text, source, label, text_clean]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coronaFake[df_coronaFake['text_clean'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a038bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db0a9ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '000km', '000mg', '000th']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using CountVectorizer with ngram_range = (1,1)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "cv_11 = CountVectorizer(ngram_range=(1, 1), lowercase=True)\n",
    "df_vec_cv_11 = cv_11.fit_transform(text_clean)\n",
    "cv_11.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b421f20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_cv_11.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b70b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '00 am', '00 minute', '00 pay', '00 pm']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using CountVectorizer with ngram_range = (1,2)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "cv_12 = CountVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "df_vec_cv_12 = cv_12.fit_transform(text_clean)\n",
    "cv_12.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "699c877a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_cv_12.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1872ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '00 am', '00 am every', '00 minute', '00 minute mark']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using CountVectorizer with ngram_range = (1,3)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "cv_13 = CountVectorizer(ngram_range=(1, 3), lowercase=True)\n",
    "df_vec_cv_13 = cv_13.fit_transform(text_clean)\n",
    "cv_13.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da5da77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_cv_13.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8b936f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '000km', '000mg', '000th']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using TfidfVectorizer with ngram_range = (1,1)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "tfid_11 = TfidfVectorizer(ngram_range=(1, 1), lowercase=True)\n",
    "df_vec_tfid_11 = tfid_11.fit_transform(text_clean)\n",
    "tfid_11.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ec2572c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_tfid_11.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22b0f13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '00 am', '00 minute', '00 pay', '00 pm']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using TfidfVectorizer with ngram_range = (1,2)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "tfid_12 = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "df_vec_tfid_12 = tfid_12.fit_transform(text_clean)\n",
    "tfid_12.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe8dc631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_tfid_12.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4598bbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '00 am', '00 am every', '00 minute', '00 minute mark']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform data using TfidfVectorizer with ngram_range = (1,3)\n",
    "\n",
    "text_clean = df_coronaFake[\"text_clean\"]\n",
    "tfid_13 = TfidfVectorizer(ngram_range=(1, 3), lowercase=True)\n",
    "df_vec_tfid_13 = tfid_13.fit_transform(text_clean)\n",
    "tfid_13.get_feature_names()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74cde98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vec_tfid_13.A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd77ff",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236b193",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62b8ea",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a07b2",
   "metadata": {},
   "source": [
    "[20 points] Now, let’s use sklearn.linear_model.LogisticRegressionCV\n",
    "to do some predictions. \n",
    "\n",
    "Set cv = 5, random_state = 265, and max_iter =\n",
    "1000, and n_jobs = -1 (other parameters should be left as default) \n",
    "\n",
    "[Note: training\n",
    "size is 70%, test size is 30%, split by random_state = 265].\n",
    "\n",
    "    a. [10 points] By using the three (3) different versions of the CountVectorizer\n",
    "    dataset you created above, run logistic regression to predict class labels (fake,\n",
    "    true). Report three (3) accuracy values associated with each of the regressions.\n",
    "    \n",
    "    b. [10 points] By using the three (3) different versions of the TfidfVectorizer\n",
    "    dataset you created above, run logistic regression to predict class labels (fake,\n",
    "    true). Report three (3) accuracy values associated with each of the regressions.\n",
    "    \n",
    "    c. Combine and report all accuracy values in a table (6 values in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80b90934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e44f8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and create LogisticRegressionCV model\n",
    "\n",
    "X = df_coronaFake[\"text_clean\"]\n",
    "y = df_coronaFake[\"label\"]\n",
    "logreg_cv = LogisticRegressionCV(cv = 5, random_state = 265, max_iter = 1000, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8933b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9075144508670521\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with CountVectorizer with ngram_range = (1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = cv_11.fit_transform(X_train)\n",
    "X_test = cv_11.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05614d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9132947976878613\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with CountVectorizer with ngram_range = (1,2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = cv_12.fit_transform(X_train)\n",
    "X_test = cv_12.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e959962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8988439306358381\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with CountVectorizer with ngram_range = (1,3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = cv_13.fit_transform(X_train)\n",
    "X_test = cv_13.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d563a4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9248554913294798\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with TFIDVectorizer with ngram_range = (1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = tfid_11.fit_transform(X_train)\n",
    "X_test = tfid_11.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03e439fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9190751445086706\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with TFIDVectorizer with ngram_range = (1,2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = tfid_12.fit_transform(X_train)\n",
    "X_test = tfid_12.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0750d72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9161849710982659\n"
     ]
    }
   ],
   "source": [
    "# split data into training and testing sets, predict using data transformed with TFIDVectorizer with ngram_range = (1,3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 265)\n",
    "X_train = tfid_13.fit_transform(X_train)\n",
    "X_test = tfid_13.transform(X_test)\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "print(accuracy_score(y_test, logreg_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa831afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies in tabular format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2c0dd",
   "metadata": {},
   "source": [
    "| Vectorizer| | Accuracy |\n",
    "| :- | :- | :-: |\n",
    "| CountVectorizer |ngram_range = (1,1)  | 0.9075144508670521 | \n",
    "|  |ngram_range = (1,2)  | 0.9132947976878613 |\n",
    "|  |ngram_range = (1,3)  | 0.8988439306358381 |\n",
    "| TfidfVectorizer |ngram_range = (1,1)  | 0.9248554913294798 | \n",
    "|  |ngram_range = (1,2)  | 0.9190751445086706 |\n",
    "|  |ngram_range = (1,3)  | 0.9161849710982659|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d553c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44328d1",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9067bf1a",
   "metadata": {},
   "source": [
    "[40 points] Check the optimizer (solver) functions used by\n",
    "sklearn.linear_model.LogisticRegressionCV. For each function, explain\n",
    "in around 100 words what they mean; specifically:\n",
    "\n",
    "    a. [8 points] What does newton-cg mean?\n",
    "    b. [8 points] What does lbfgs mean?\n",
    "    c. [8 points] What does liblinear mean?\n",
    "    d. [8 points] What does sag mean?\n",
    "    e. [8 points] What does saga mean?\n",
    "    \n",
    "Note: For this question you might need to do some online research. It is your job to find\n",
    "out how they work. You are also welcome to use formulas / matrices in your description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a65d8",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fe6d1",
   "metadata": {},
   "source": [
    "newton-cg: CG stands for conjugate gradient. It is one of the algorithms that can be used in optimization problems.  Newton CG does quadratic approximations using iterative methods for solving nonlinear optimization problems. It calculates Hessian explicitly which can be computationally expensive in high dimensions. The Hessian is a square matrix of second-order partial derivatives of order n X n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847cde9",
   "metadata": {},
   "source": [
    "lbfgs: lbfgs stands for Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm. It is an analogue of the Newton’s Method. Here the Hessian matrix is approximated using updates specified by the gradient evaluations i.e. approximate gradient evaluations. It uses an estimation to the inverse Hessian matrix. It stores only a few vectors that represent the approximation implicitly hence \"Limited-memory\". It performs well when the dataset is small compared to other algorithms. However, if not used properly, it may not converge to anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076627d6",
   "metadata": {},
   "source": [
    "liblinear: It is a \"library for large linear classifications\". The solver uses Coordinate Descent algorithm. Coordinate Descent algorith solves optimization problems by successively performing approximate minimization along coordinate directions/coordinate hyperplanes. There are a few drawbacks of this solver, it may get stuck at a non-stationary point if the level curves of a function are not smooth, it cannot run in parallel and it cannot learn a true multinomial/multiclass model (the optimization problem is decomposed in a “one-vs-rest” fashion, so separate binary classifiers are trained for all classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0eda2",
   "metadata": {},
   "source": [
    "sag: It stands for Stochastic Average Gradient. This method optimizes the sum of a limited number of smooth convex functions. The iteration cost is independent of the number of terms in the sum, similar to stochastic gradient descent. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate compared to stochastic gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963998c",
   "metadata": {},
   "source": [
    "saga: It is a variant of SAG. It supports the non-smooth penalty L1 option i.e. L1 Regularization. It is commonly used for sparse multinomial logistic regression. It is also great for dealing with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139a0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
